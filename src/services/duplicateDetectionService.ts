/**
 * å»é‡æ£€æµ‹æœåŠ¡
 * ä½¿ç”¨TF-IDFå’Œä½™å¼¦ç›¸ä¼¼åº¦æ£€æµ‹å†…å®¹é‡å¤
 * é˜²æ­¢Programmatic SEOç”Ÿæˆè¿‡å¤šç›¸ä¼¼å†…å®¹å¯¼è‡´æœç´¢å¼•æ“æƒ©ç½š
 */

import { createClient } from '@supabase/supabase-js'

// å…¼å®¹Viteå’ŒNodeç¯å¢ƒ
const getEnv = (key: string): string => {
  if (typeof import.meta !== 'undefined' && import.meta.env) {
    return import.meta.env[key] || ''
  }
  return process.env[key] || ''
}

const supabaseUrl = getEnv('VITE_SUPABASE_URL')
const supabaseKey = getEnv('VITE_SUPABASE_ANON_KEY')
const supabase = createClient(supabaseUrl, supabaseKey)

export interface DuplicateCheckRequest {
  templateId: string          // è§†é¢‘æ¨¡æ¿ID
  language: string             // è¯­è¨€
  newContent: string           // æ–°ç”Ÿæˆçš„å†…å®¹
  compareWithAll?: boolean     // æ˜¯å¦ä¸æ‰€æœ‰å·²æœ‰å†…å®¹æ¯”è¾ƒï¼ˆé»˜è®¤falseï¼Œä»…åŒæ¨¡æ¿åŒè¯­è¨€ï¼‰
}

export interface DuplicateCheckResult {
  isDuplicate: boolean         // æ˜¯å¦é‡å¤ï¼ˆç›¸ä¼¼åº¦>70%ï¼‰
  maxSimilarity: number        // æœ€é«˜ç›¸ä¼¼åº¦
  similarPages: SimilarPage[]  // ç›¸ä¼¼é¡µé¢åˆ—è¡¨
  threshold: number            // åˆ¤å®šé˜ˆå€¼
  checkedCount: number         // æ£€æŸ¥çš„é¡µé¢æ•°é‡
}

export interface SimilarPage {
  pageId: string
  targetKeyword: string
  similarity: number           // ç›¸ä¼¼åº¦0-1
  contentPreview: string       // å†…å®¹é¢„è§ˆ
  url: string                  // é¡µé¢URL
}

export interface BatchDuplicateCheckRequest {
  templateId: string
  language: string
  contents: Array<{
    id: string               // ä¸´æ—¶IDï¼ˆç”¨äºè¯†åˆ«ï¼‰
    keyword: string
    content: string
  }>
}

export interface BatchDuplicateCheckResult {
  duplicates: Array<{
    id: string
    keyword: string
    isDuplicate: boolean
    maxSimilarity: number
    similarTo: string[]      // ç›¸ä¼¼çš„å…¶ä»–IDæˆ–keyword
  }>
  overallDuplicateRate: number  // æ€»ä½“é‡å¤ç‡
}

class DuplicateDetectionService {
  private readonly DUPLICATE_THRESHOLD = 0.70  // 70%ç›¸ä¼¼åº¦è§†ä¸ºé‡å¤
  private readonly WARNING_THRESHOLD = 0.50    // 50%ç›¸ä¼¼åº¦å‘å‡ºè­¦å‘Š

  /**
   * æ£€æŸ¥å•ä¸ªå†…å®¹æ˜¯å¦é‡å¤
   */
  async checkDuplicate(request: DuplicateCheckRequest): Promise<DuplicateCheckResult> {
    console.log(`\n[DuplicateDetect] ğŸ” å¼€å§‹æ£€æµ‹é‡å¤...`)
    console.log(`[DuplicateDetect] æ¨¡æ¿ID: ${request.templateId}`)
    console.log(`[DuplicateDetect] è¯­è¨€: ${request.language}`)

    // 1. è·å–å·²æœ‰å†…å®¹
    const existingPages = await this.loadExistingPages(
      request.templateId,
      request.language,
      request.compareWithAll
    )

    console.log(`[DuplicateDetect] å·²åŠ è½½ ${existingPages.length} ä¸ªå·²æœ‰é¡µé¢`)

    if (existingPages.length === 0) {
      console.log(`[DuplicateDetect] âœ… æ— å·²æœ‰é¡µé¢ï¼Œè·³è¿‡æ£€æµ‹`)
      return {
        isDuplicate: false,
        maxSimilarity: 0,
        similarPages: [],
        threshold: this.DUPLICATE_THRESHOLD,
        checkedCount: 0
      }
    }

    // 2. è®¡ç®—ç›¸ä¼¼åº¦
    const similarities: SimilarPage[] = []

    for (const page of existingPages) {
      const similarity = this.calculateSimilarity(
        request.newContent,
        page.guide_content
      )

      if (similarity > this.WARNING_THRESHOLD) {
        similarities.push({
          pageId: page.id,
          targetKeyword: page.target_keyword,
          similarity,
          contentPreview: page.guide_content.slice(0, 200),
          url: `/${page.language}/guide/${page.content_template?.slug || 'template'}/${page.keyword_slug}`
        })
      }
    }

    // 3. æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort((a, b) => b.similarity - a.similarity)

    const maxSimilarity = similarities.length > 0 ? similarities[0].similarity : 0
    const isDuplicate = maxSimilarity > this.DUPLICATE_THRESHOLD

    console.log(`[DuplicateDetect] ğŸ“Š æ£€æµ‹ç»“æœ:`)
    console.log(`[DuplicateDetect] æœ€é«˜ç›¸ä¼¼åº¦: ${(maxSimilarity * 100).toFixed(2)}%`)
    console.log(`[DuplicateDetect] æ˜¯å¦é‡å¤: ${isDuplicate ? 'æ˜¯' : 'å¦'}`)
    console.log(`[DuplicateDetect] ç›¸ä¼¼é¡µé¢æ•°: ${similarities.length}`)

    return {
      isDuplicate,
      maxSimilarity,
      similarPages: similarities.slice(0, 10), // æœ€å¤šè¿”å›10ä¸ª
      threshold: this.DUPLICATE_THRESHOLD,
      checkedCount: existingPages.length
    }
  }

  /**
   * æ‰¹é‡æ£€æµ‹é‡å¤
   */
  async checkBatchDuplicates(request: BatchDuplicateCheckRequest): Promise<BatchDuplicateCheckResult> {
    console.log(`\n[DuplicateDetect Batch] ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹é‡å¤...`)
    console.log(`[DuplicateDetect Batch] å†…å®¹æ•°é‡: ${request.contents.length}`)

    // 1. è·å–å·²æœ‰å†…å®¹
    const existingPages = await this.loadExistingPages(
      request.templateId,
      request.language,
      false
    )

    console.log(`[DuplicateDetect Batch] å·²åŠ è½½ ${existingPages.length} ä¸ªå·²æœ‰é¡µé¢`)

    const results: BatchDuplicateCheckResult['duplicates'] = []

    // 2. æ£€æµ‹æ¯ä¸ªæ–°å†…å®¹ä¸å·²æœ‰å†…å®¹çš„ç›¸ä¼¼åº¦
    for (const content of request.contents) {
      let maxSimilarity = 0
      const similarTo: string[] = []

      // ä¸å·²æœ‰é¡µé¢æ¯”è¾ƒ
      for (const page of existingPages) {
        const similarity = this.calculateSimilarity(content.content, page.guide_content)

        if (similarity > maxSimilarity) {
          maxSimilarity = similarity
        }

        if (similarity > this.DUPLICATE_THRESHOLD) {
          similarTo.push(page.target_keyword)
        }
      }

      // ä¸åŒæ‰¹æ¬¡å…¶ä»–å†…å®¹æ¯”è¾ƒ
      for (const other of request.contents) {
        if (other.id === content.id) continue

        const similarity = this.calculateSimilarity(content.content, other.content)

        if (similarity > maxSimilarity) {
          maxSimilarity = similarity
        }

        if (similarity > this.DUPLICATE_THRESHOLD) {
          similarTo.push(other.keyword)
        }
      }

      results.push({
        id: content.id,
        keyword: content.keyword,
        isDuplicate: maxSimilarity > this.DUPLICATE_THRESHOLD,
        maxSimilarity,
        similarTo
      })
    }

    // 3. è®¡ç®—æ€»ä½“é‡å¤ç‡
    const duplicateCount = results.filter(r => r.isDuplicate).length
    const overallDuplicateRate = duplicateCount / results.length

    console.log(`[DuplicateDetect Batch] ğŸ“Š æ‰¹é‡æ£€æµ‹ç»“æœ:`)
    console.log(`[DuplicateDetect Batch] é‡å¤æ•°é‡: ${duplicateCount}/${results.length}`)
    console.log(`[DuplicateDetect Batch] é‡å¤ç‡: ${(overallDuplicateRate * 100).toFixed(2)}%`)

    return {
      duplicates: results,
      overallDuplicateRate
    }
  }

  /**
   * è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
   */
  private calculateSimilarity(text1: string, text2: string): number {
    // 1. é¢„å¤„ç†æ–‡æœ¬
    const tokens1 = this.tokenize(text1)
    const tokens2 = this.tokenize(text2)

    if (tokens1.length === 0 || tokens2.length === 0) {
      return 0
    }

    // 2. æ„å»ºè¯æ±‡è¡¨
    const vocabulary = new Set([...tokens1, ...tokens2])

    // 3. è®¡ç®—TFï¼ˆè¯é¢‘ï¼‰
    const tf1 = this.calculateTF(tokens1, vocabulary)
    const tf2 = this.calculateTF(tokens2, vocabulary)

    // 4. è®¡ç®—IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
    const idf = this.calculateIDF([tokens1, tokens2], vocabulary)

    // 5. è®¡ç®—TF-IDFå‘é‡
    const tfidf1 = this.calculateTFIDF(tf1, idf)
    const tfidf2 = this.calculateTFIDF(tf2, idf)

    // 6. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    const similarity = this.cosineSimilarity(tfidf1, tfidf2)

    return similarity
  }

  /**
   * åˆ†è¯ï¼ˆç®€å•å®ç°ï¼‰
   */
  private tokenize(text: string): string[] {
    // è½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹ï¼Œåˆ†è¯
    return text
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ')
      .split(/\s+/)
      .filter(token => token.length > 2) // è¿‡æ»¤çŸ­è¯
  }

  /**
   * è®¡ç®—TFï¼ˆè¯é¢‘ï¼‰
   */
  private calculateTF(tokens: string[], vocabulary: Set<string>): Map<string, number> {
    const tf = new Map<string, number>()
    const totalTokens = tokens.length

    for (const word of vocabulary) {
      const count = tokens.filter(t => t === word).length
      tf.set(word, count / totalTokens)
    }

    return tf
  }

  /**
   * è®¡ç®—IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
   */
  private calculateIDF(documents: string[][], vocabulary: Set<string>): Map<string, number> {
    const idf = new Map<string, number>()
    const totalDocs = documents.length

    for (const word of vocabulary) {
      const docsWithWord = documents.filter(doc => doc.includes(word)).length
      idf.set(word, Math.log((totalDocs + 1) / (docsWithWord + 1)) + 1) // å¹³æ»‘IDF
    }

    return idf
  }

  /**
   * è®¡ç®—TF-IDFå‘é‡
   */
  private calculateTFIDF(tf: Map<string, number>, idf: Map<string, number>): Map<string, number> {
    const tfidf = new Map<string, number>()

    for (const [word, tfValue] of tf.entries()) {
      const idfValue = idf.get(word) || 0
      tfidf.set(word, tfValue * idfValue)
    }

    return tfidf
  }

  /**
   * è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
   */
  private cosineSimilarity(vector1: Map<string, number>, vector2: Map<string, number>): number {
    let dotProduct = 0
    let magnitude1 = 0
    let magnitude2 = 0

    // è®¡ç®—æ‰€æœ‰ç»´åº¦
    const allKeys = new Set([...vector1.keys(), ...vector2.keys()])

    for (const key of allKeys) {
      const v1 = vector1.get(key) || 0
      const v2 = vector2.get(key) || 0

      dotProduct += v1 * v2
      magnitude1 += v1 * v1
      magnitude2 += v2 * v2
    }

    if (magnitude1 === 0 || magnitude2 === 0) {
      return 0
    }

    return dotProduct / (Math.sqrt(magnitude1) * Math.sqrt(magnitude2))
  }

  /**
   * ä»æ•°æ®åº“åŠ è½½å·²æœ‰é¡µé¢
   */
  private async loadExistingPages(
    templateId: string,
    language: string,
    compareWithAll: boolean = false
  ) {
    let query = getSupabase()
      .from('seo_page_variants')
      .select(`
        id,
        target_keyword,
        keyword_slug,
        guide_content,
        language,
        content_template:content_template_id (
          slug
        )
      `)
      .eq('is_published', true)

    // å¦‚æœä¸æ˜¯å…¨å±€æ¯”è¾ƒï¼Œåªæ¯”è¾ƒåŒæ¨¡æ¿åŒè¯­è¨€
    if (!compareWithAll) {
      query = query
        .eq('template_id', templateId)
        .eq('language', language)
    } else {
      query = query.eq('language', language)
    }

    const { data, error } = await query

    if (error) {
      console.error('[DuplicateDetect] åŠ è½½å·²æœ‰é¡µé¢å¤±è´¥:', error)
      throw new Error('åŠ è½½å·²æœ‰é¡µé¢å¤±è´¥')
    }

    return data || []
  }

  /**
   * æ›´æ–°é¡µé¢çš„ç›¸ä¼¼åº¦å¾—åˆ†
   */
  async updateSimilarityScore(
    pageId: string,
    similarity: number,
    isDuplicate: boolean
  ): Promise<void> {
    const { error } = await getSupabase()
      .from('seo_page_variants')
      .update({
        content_similarity_score: similarity,
        is_duplicate: isDuplicate
      })
      .eq('id', pageId)

    if (error) {
      console.error('[DuplicateDetect] æ›´æ–°ç›¸ä¼¼åº¦å¤±è´¥:', error)
      throw new Error('æ›´æ–°ç›¸ä¼¼åº¦å¤±è´¥')
    }
  }

  /**
   * è·å–é‡å¤å†…å®¹ç»Ÿè®¡
   */
  async getDuplicateStats(templateId: string): Promise<{
    totalPages: number
    duplicatePages: number
    duplicateRate: number
    averageSimilarity: number
  }> {
    const { data, error } = await getSupabase()
      .from('seo_page_variants')
      .select('is_duplicate, content_similarity_score')
      .eq('template_id', templateId)
      .eq('is_published', true)

    if (error) {
      throw new Error('è·å–ç»Ÿè®¡å¤±è´¥')
    }

    const pages = data || []
    const totalPages = pages.length
    const duplicatePages = pages.filter(p => p.is_duplicate).length
    const duplicateRate = totalPages > 0 ? duplicatePages / totalPages : 0

    const avgSimilarity = pages.reduce((sum, p) => sum + (p.content_similarity_score || 0), 0) / totalPages

    return {
      totalPages,
      duplicatePages,
      duplicateRate,
      averageSimilarity
    }
  }

  /**
   * ç®€å•çš„Jaccardç›¸ä¼¼åº¦ï¼ˆå¤‡ç”¨ç®—æ³•ï¼Œæ›´å¿«ä½†ä¸å¤ªç²¾ç¡®ï¼‰
   */
  calculateJaccardSimilarity(text1: string, text2: string): number {
    const tokens1 = new Set(this.tokenize(text1))
    const tokens2 = new Set(this.tokenize(text2))

    const intersection = new Set([...tokens1].filter(t => tokens2.has(t)))
    const union = new Set([...tokens1, ...tokens2])

    return intersection.size / union.size
  }

  /**
   * å¿«é€ŸæŒ‡çº¹æ£€æµ‹ï¼ˆç”¨äºåˆæ­¥ç­›é€‰ï¼‰
   */
  generateFingerprint(text: string): string {
    // ç”Ÿæˆæ–‡æœ¬çš„çŸ­æŒ‡çº¹ï¼ˆç”¨äºå¿«é€Ÿæ¯”è¾ƒï¼‰
    const tokens = this.tokenize(text)
    const sorted = [...new Set(tokens)].sort()
    const topWords = sorted.slice(0, 50) // å–å‰50ä¸ªå”¯ä¸€è¯
    return topWords.join('|')
  }

  /**
   * æ£€æµ‹ç»“æ„ç›¸ä¼¼åº¦ï¼ˆH2æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
   */
  calculateStructuralSimilarity(text1: string, text2: string): number {
    const h2Regex = /^##\s+(.+)$/gm

    const h2s1 = [...text1.matchAll(h2Regex)].map(m => m[1].toLowerCase())
    const h2s2 = [...text2.matchAll(h2Regex)].map(m => m[1].toLowerCase())

    if (h2s1.length === 0 || h2s2.length === 0) {
      return 0
    }

    // è®¡ç®—æ ‡é¢˜é›†åˆçš„Jaccardç›¸ä¼¼åº¦
    const set1 = new Set(h2s1)
    const set2 = new Set(h2s2)

    const intersection = new Set([...set1].filter(h => set2.has(h)))
    const union = new Set([...set1, ...set2])

    return intersection.size / union.size
  }
}

export const duplicateDetectionService = new DuplicateDetectionService()
export default duplicateDetectionService
